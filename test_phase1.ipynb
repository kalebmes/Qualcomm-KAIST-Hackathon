{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3993b52-7522-4b74-800a-2f7a4bef8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from transformers import *\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af1c42-9f8c-4bed-a6e9-47300f8f223e",
   "metadata": {},
   "source": [
    "# 1. Test data Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93bd13b4-bc94-4032-a76e-c0bacbd7b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "df = pd.read_csv('./data/hackathon_test_for_user.csv', encoding='cp949')\n",
    "# df = pd.read_excel('./data_phase2/test_data.xlsx', sheet_name='Sheet1', index_col=0)\n",
    "df['Age_float'] = df['Age']\n",
    "# df = df.sort_values(by=['User_ID'])\n",
    "\n",
    "# df['Gender'] = df['Gender'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['Age'], prefix='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02f07ee5-b56c-4259-8b5d-bb1ce44c5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3600 entries, 1 to 12749\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   User_ID             3600 non-null   object\n",
      " 1   Gender              3600 non-null   object\n",
      " 2   Q_number            3600 non-null   object\n",
      " 3   Age_float           3600 non-null   object\n",
      " 4   cat_20              3600 non-null   object\n",
      " 5   cat_30              3600 non-null   object\n",
      " 6   cat_40              3600 non-null   object\n",
      " 7   Long_Answer_concat  3600 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 253.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "857a5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now concatenate the \"Long_Answer\" of k similar user_ids\n",
    "\n",
    "# new_columns = [col for col in df.columns if col not in ['Short_Answer', 'Long_Answer']]\n",
    "# new_df = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "# num_samples = df.shape[0]\n",
    "# k_user_ids = 2\n",
    "# # print(df['Long_Answer'])\n",
    "# for i in range(0, num_samples, k_user_ids):\n",
    "    \n",
    "#     user_ids = df.iloc[i:i+k_user_ids]['User_ID'].to_list()\n",
    "    \n",
    "#     # check if all the user_ids are the same\n",
    "#     if len(set(user_ids)) == 1:\n",
    "        \n",
    "#         # concatenate the \"Long_Answer\" of k similar user_ids\n",
    "\n",
    "#         long_answer_concat = df.iloc[i:i+k_user_ids]['Long_Answer'].to_list()\n",
    "        \n",
    "#         concat_str = ' 그리고 '.join(long_answer_concat)\n",
    "        \n",
    "#         # print(concat_str)\n",
    "        \n",
    "#         # since the \"User_ID\", and all other elements are the same, we can just take the first one\n",
    "#         new_row = df.iloc[i:i+k_user_ids].head(1)[new_columns]\n",
    "#         new_row['Long_Answer_concat'] = concat_str\n",
    "        \n",
    "#         new_df = pd.concat([new_df, new_row], axis=0)\n",
    "    \n",
    "\n",
    "# # print(new_df)\n",
    "# # print(df.shape)\n",
    "# assert new_df.shape[0] == df.shape[0] // k_user_ids\n",
    "# assert new_df.shape[1] == df.shape[1] - 1\n",
    "\n",
    "# df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af624420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3600 entries, 1 to 12749\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   User_ID             3600 non-null   object\n",
      " 1   Gender              3600 non-null   object\n",
      " 2   Q_number            3600 non-null   object\n",
      " 3   Age_float           3600 non-null   object\n",
      " 4   cat_20              3600 non-null   object\n",
      " 5   cat_30              3600 non-null   object\n",
      " 6   cat_40              3600 non-null   object\n",
      " 7   Long_Answer_concat  3600 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 253.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ca8334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'kykim/bert-kor-base'\n",
    "# model_name = 'monologg/kobigbird-bert-base'\n",
    "# model_name = 'beomi/kcbert-base'\n",
    "model_name = 'snunlp/KR-BERT-char16424'\n",
    "# model_name = 'klue/bert-base'\n",
    "# model_name = 'klue/roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5514041-8362-4293-a262-4f1cf6469495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "def get_model(model_name):\n",
    "    # * Model          | Tokenizer          | Pretrained weights shortcut\n",
    "    # MODEL=(DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    n_hl = model.config.num_hidden_layers\n",
    "    # embed_dim = model.config.embedding_size\n",
    "    embed_dim = 768\n",
    "    return model, tokenizer, n_hl, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85227418-0a12-4aef-b159-0ee3b20b002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/kaleb/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/pytorch_model.bin\n",
      "Some weights of the model checkpoint at snunlp/KR-BERT-char16424 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at snunlp/KR-BERT-char16424.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, n_hl, embed_dim = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66704ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['One_word_answer'] = df['Answer'].apply(lambda x: (x.split('>')[0])[x.index('<')+1:])\n",
    "# # df['One_word_answer'] = df['One_word_answer'].astype('category').cat.codes\n",
    "# # df['One_word_answer'].unique()\n",
    "\n",
    "# df['One_word_ans_enc'] = df['One_word_answer'].apply(lambda x : 1 if x.startswith('중') else (2 if x.startswith('아') or x.startswith('어') else 0))\n",
    "\n",
    "# df['Answer_rep'] = df['Answer'].apply(lambda x: x.replace(f\"<{(x.split('>')[0])[x.index('<')+1:]}>\", ''))\n",
    "\n",
    "# # df = pd.get_dummies(df, columns=['One_word_ans_enc'], prefix='is')\n",
    "# print(df['One_word_answer'].unique())\n",
    "# # df['One_word_answer'].unique() # array(['아니다', '중립', '그렇다', '아니오', '중간', '보통', '아니요']\n",
    "# # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf1f9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from konlpy.tag import Okt\n",
    "# from hanspell import spell_checker\n",
    "# import json\n",
    "# from num2words import num2words\n",
    "# from kss import split_sentences\n",
    "\n",
    "# def sentence_segmentation(text):\n",
    "#     sentences = split_sentences(text)\n",
    "#     return sentences\n",
    "\n",
    "# def pos_tagging(text):\n",
    "#     okt = Okt()\n",
    "#     tagged = okt.pos(text)\n",
    "#     return tagged\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess_text(text, apply_ngrams=False, ngram_size=1, apply_replacements=True, apply_normalization=True, remove_stopwords=False):\n",
    "#     # Check and correct spelling\n",
    "#     # checked_spelling = spell_checker.check(text)\n",
    "#     # text = checked_spelling.checked\n",
    "\n",
    "#     # Remove special characters using regular expressions\n",
    "#     # text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]\", \" \", text)\n",
    "\n",
    "#     # Initialize the Okt tokenizer\n",
    "#     okt = Okt()\n",
    "\n",
    "#     # Tokenize the text into words\n",
    "#     # words = okt.morphs(text, stem=False)\n",
    "#     words = text.split()\n",
    "#     # print(words)\n",
    "\n",
    "#     # if apply_ner:\n",
    "#     #     # Perform named entity recognition\n",
    "#     #     entities = okt.nouns(text)\n",
    "#     #     words = [\"[NER]\" if word in entities else word for word in words]\n",
    "\n",
    "#     if apply_replacements:\n",
    "#         # Replace specific words or phrases (example)\n",
    "#         replacements = {\"아니다\": \"아닙니다\", \"그렇다\": \"그렇습니다\", \"이다\": \"입니다\"}\n",
    "#         words = [replacements.get(word, word) for word in words]\n",
    "\n",
    "#     if apply_normalization:\n",
    "#         # Perform text normalization \n",
    "#         words = [num2words(word) if word.isdigit() else word for word in words]\n",
    "#         words = [okt.normalize(word) for word in words]\n",
    "#         # words = [normalization_map.get(word, word) for word in words]\n",
    "\n",
    "#     # Remove stopwords\n",
    "#     if remove_stopwords:\n",
    "#         stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '다']\n",
    "#         words = [word for word in words if word not in stopwords]\n",
    "\n",
    "#     # Remove short and long words\n",
    "#     # words = [word for word in words if 3 <= len(word) <= 30]\n",
    "\n",
    "#     if apply_ngrams:\n",
    "#         # Generate N-grams\n",
    "#         ngrams = zip(*[words[i:] for i in range(ngram_size)])\n",
    "#         ngrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "#         words.extend(ngrams)\n",
    "\n",
    "#     return \" \".join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb12877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['preprocessed_ans'] = df['Answer_rep'].apply(preprocess_text)\n",
    "df['Answer_rep'] = df['Answer'].apply(lambda x: x.replace(f\"<{(x.split('>')[0])[x.index('<')+1:]}>\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7885c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaleb/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# if there are any columns that you wish to tokenize with bert, add them here\n",
    "# if you are eliminating the one-word-short answer -> please modify Answer to Answer_rep\n",
    "# tensor = tokenizer(df['Long_Answer_concat'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True)\n",
    "tensor = tokenizer(df['Answer_rep'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f2880f3-276c-43fe-8a89-ce1e385007a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMapDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {k:v[idx] for k,v in self.data.items()}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9a40604-1a9b-481e-9416-0f21438a3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyMapDataset(tensor)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8036030-776e-40a4-a3cf-88fac9261d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, dl, device=0):\n",
    "    pooled = []\n",
    "    # hidden = []\n",
    "    model.cuda(device)\n",
    "    model.eval()\n",
    "    for data in dl:\n",
    "        data = {k:v.cuda(device) for k,v in data.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**data, output_hidden_states=False)\n",
    "        p = output.pooler_output#, output.hidden_states\n",
    "        pooled.append(p) # pooler output\n",
    "        # hidden.append(h[-1][:,0,:]) # only [CLS] token embedding \n",
    "    return torch.cat(pooled)#, torch.cat(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fe5af83-9a56-40ff-80af-e861e53ab88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forward(model, dl, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a590e03-5aa7-47e3-8a11-57534cc7ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[0].shape, result[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2caa4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# from cuml.metrics import accuracy_score, roc_auc_score\n",
    "# # from sklearn.metrics import roc_auc_score, \n",
    "\n",
    "# class CuMLVotingClassifier:\n",
    "#     def __init__(self, estimators, voting='hard'):\n",
    "#         self.estimators = estimators\n",
    "#         self.n_estimators = len(estimators)\n",
    "#         self.voting = voting\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         for _, estimator in self.estimators:\n",
    "#             estimator.fit(X, y)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         if self.voting == 'hard':\n",
    "#             predictions = np.zeros((X.shape[0], self.n_estimators), dtype=np.int32)\n",
    "#             for i, (_, estimator) in enumerate(self.estimators):\n",
    "#                 predictions[:, i] = estimator.predict(X)\n",
    "#             majority_vote = np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=1, arr=predictions)\n",
    "#             return majority_vote\n",
    "#         elif self.voting == 'soft':\n",
    "#             probas = self.predict_proba(X)\n",
    "#             return np.argmax(probas, axis=1)\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         if self.voting == 'hard':\n",
    "#             raise AttributeError(\"predict_proba is not available when voting='hard'\")\n",
    "#         else:\n",
    "#             probas = np.zeros((X.shape[0], 2))\n",
    "#             for _, estimator in self.estimators:\n",
    "#                 probas += estimator.predict_proba(X)\n",
    "#             probas /= self.n_estimators\n",
    "#             return probas\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return accuracy_score(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4403b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[0].shape\n",
    "# we will concatenate the \"Gender\" and \"Age\" columns with the train, and test results\n",
    "# df['Age_encoded'] = df['Age'].astype('category').cat.codes\n",
    "# df = pd.get_dummies(df, columns=['Age'], prefix='cat')\n",
    "# added_cols = [col for col in df.columns if col.startswith('is')]\n",
    "# df['Age_normalized'] = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n",
    "# added_cols = ['Age_normalized']\n",
    "# added_cols.append('Gender')\n",
    "added_cols = ['Gender', 'Age_float'] + [col for col in df.columns if col.startswith('cat')]\n",
    "\n",
    "# added_cols = []\n",
    "nums = 3\n",
    "for i in range(nums):\n",
    "    added_cols += ['Gender', 'Age_float']\n",
    "    if i % 3 == 0:\n",
    "        added_cols += [col for col in df.columns if col.startswith('cat')]\n",
    "# for _ in range(nums):\n",
    "#     added_cols += ['Age']\n",
    "#     # added_cols += ['One_word_ans_enc']\n",
    "# for _ in range(nums//6):\n",
    "\n",
    "# random.shuffle(added_cols)\n",
    "# added_cols.append('One_word_ans_enc')\n",
    "\n",
    "# df[f'{col_name}_encoded'] = df[col_name].astype('category').cat.codes\n",
    "col_data = df[added_cols].values\n",
    "# map the col_data to have the same shape as the result[0] using nn.Linear\n",
    "# same_mapper_layer = nn.Sequential(nn.Linear(col_data.shape[1], result[0].shape[1]), nn.ReLU())\n",
    "# same_mapper_layer.cuda(0)\n",
    "# same_mapper_layer_test = nn.Sequential(nn.Linear(test_col_data.shape[1], test_result[0].shape[1]), nn.ReLU())\n",
    "# same_mapper_layer_test.cuda(1)\n",
    "# # convert them to tensor\n",
    "col_data = col_data.astype(float)\n",
    "col_data = torch.tensor(col_data, dtype=torch.float32).cuda(0)\n",
    "\n",
    "# col_data = same_mapper_layer(col_data)\n",
    "# test_col_data = same_mapper_layer_test(test_col_data)\n",
    "# concatenate it with the tensor\n",
    "result_added = torch.cat([result, col_data], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec2d55-0dcf-47d7-9f19-727365f2cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7200, 100])\n",
      "torch.Size([7200, 868])\n",
      "['Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender', 'Gender', 'cat_20', 'cat_30', 'cat_40', 'Gender', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "print(col_data.shape)\n",
    "print(result_added.shape)\n",
    "print(added_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed8543-7914-46df-bfa3-4ab2711ad13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward datasets\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "    \n",
    "# ds = MyDataset(result[0])\n",
    "# dl = DataLoader(ds, shuffle=False, batch_size=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b060b-62e3-4109-8049-4147d102c1da",
   "metadata": {},
   "source": [
    "# Load each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443e262-4392-4db2-aa3c-adc26dc4b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie_path = './ckpt/IE/epoch_81.pt'\n",
    "# sn_path = './ckpt/SN/epoch_30.pt'\n",
    "# tf_path = './ckpt/TF/epoch0.pt'\n",
    "# jp_path = './ckpt/jp/epoch_25.pt'\n",
    "# test_number = 19\n",
    "# # import csv\n",
    "# from torch import nn \n",
    "MBTI = ['IE', 'SN', 'TF', 'JP']\n",
    "# chkpt_path = [f'./ckpt_exp{test_number}/{name}/best.pt' for name in MBTI]\n",
    "predictions = list()\n",
    "mlp = nn.Sequential(nn.Linear(col_data.shape[1], 786//2),\n",
    "                          nn.ReLU(),        \n",
    "                          nn.Linear(786//2, 786),\n",
    "                          )\n",
    "\n",
    "import joblib\n",
    "result_dir = '/root/QIA2023_phase1'\n",
    "# for i in range(4):\n",
    "mlp = mlp.cuda(0)\n",
    "\n",
    "test_number = 204 # might be changed according to the unique number we want to save\n",
    "mlp.load_state_dict(torch.load(f'{result_dir}/{MBTI[0]}_model_{test_number}.pth'))\n",
    "mlp_output_train = mlp(col_data)\n",
    "concatenated_features = np.concatenate([mlp_output_train.detach().cpu().numpy(), result[0].detach().cpu().numpy()], axis=1)\n",
    "clf = joblib.load(f'{result_dir}/{MBTI[0]}_svm_{test_number}.pkl')\n",
    "prob_predic = clf.predict_proba(concatenated_features)[:, 1]\n",
    "predictions.append(prob_predic)\n",
    "\n",
    "test_number = 200\n",
    "mlp.load_state_dict(torch.load(f'{result_dir}/{MBTI[1]}_model_{test_number}.pth'))\n",
    "mlp_output_train = mlp(col_data)\n",
    "concatenated_features = np.concatenate([mlp_output_train.detach().cpu().numpy(), result[0].detach().cpu().numpy()], axis=1)\n",
    "clf = joblib.load(f'{result_dir}/{MBTI[1]}_svm_{test_number}.pkl')\n",
    "prob_predic = clf.predict_proba(concatenated_features)[:, 1]\n",
    "predictions.append(prob_predic)\n",
    "\n",
    "test_number = 201\n",
    "mlp.load_state_dict(torch.load(f'{result_dir}/{MBTI[2]}_model_{test_number}.pth'))\n",
    "mlp_output_train = mlp(col_data)\n",
    "concatenated_features = np.concatenate([mlp_output_train.detach().cpu().numpy(), result[0].detach().cpu().numpy()], axis=1)\n",
    "clf = joblib.load(f'{result_dir}/{MBTI[2]}_svm_{test_number}.pkl')\n",
    "prob_predic = clf.predict_proba(concatenated_features)[:, 1]\n",
    "predictions.append(prob_predic)\n",
    "    \n",
    "test_number = 200\n",
    "mlp.load_state_dict(torch.load(f'{result_dir}/{MBTI[3]}_model_{test_number}.pth'))\n",
    "mlp_output_train = mlp(col_data)\n",
    "concatenated_features = np.concatenate([mlp_output_train.detach().cpu().numpy(), result[0].detach().cpu().numpy()], axis=1)\n",
    "clf = joblib.load(f'{result_dir}/{MBTI[3]}_svm_{test_number}.pkl')\n",
    "prob_predic = clf.predict_proba(concatenated_features)[:, 1]\n",
    "predictions.append(prob_predic)\n",
    "    \n",
    "# print(chkpt_path)\n",
    "# path = [ie_path, sn_path, tf_path, jp_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729a66e-c321-4d88-a804-b08d618c9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(ie_path, map_location='cpu')\n",
    "len(predictions[0]), len(predictions[1]), len(predictions[2]), len(predictions[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7725f9a-d4a6-4909-87bd-dea559d6f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie, sn, tf, jp = [main(path) for path in chkpt_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749af8d-5eb3-42e9-8ff3-09c5bf3485a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\n",
    "                    'I/E': predictions[0].tolist(), \n",
    "                    'S/N': predictions[1].tolist(), \n",
    "                    'T/F': predictions[2].tolist(), \n",
    "                    'J/P': predictions[3].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6317c7-6702-4225-b8b2-f99813d4f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['idx'] = test.index + 1\n",
    "test.set_index('idx', inplace=True)\n",
    "test\n",
    "# test_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7db6f-c87c-4ee0-9662-67244408df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to assign another unique number, change the sub_number\n",
    "# sub_number_av = 'phase2_av_svm_only_full_data_trial1'\n",
    "# sub_number_av = 'phase2_av_knn_only_full_data_trial1'\n",
    "# sub_number_av = 'phase2_av_knn60pt47_only_full_data'\n",
    "# # sub_number_max = 'phase2_max'\n",
    "# csv_path_av = f'./submission_exp{sub_number_av}.csv'\n",
    "sub_number = 'phase1'\n",
    "csv_path = f'submission_exp{sub_number}.csv'\n",
    "# csv_path_max = f'./submission_exp{sub_number_max}.csv'\n",
    "# add the column named idx and start from '1'\n",
    "\n",
    "test.to_csv(csv_path)\n",
    "# print('saved to ', csv_path)\n",
    "# test_av.to_csv(csv_path_av)\n",
    "# test_max.to_csv(csv_path_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd38debc8939d20aef59f39ab68609c3347ed5c96fcef172f14e92af6cc98870"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
