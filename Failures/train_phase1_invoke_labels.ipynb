{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037b62db-a8c4-4bd5-98fb-0b09772c1747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from transformers import *\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from synonyms import generate_synonym\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd234c6-5e1d-4c26-8bb7-f17cf7478798",
   "metadata": {},
   "source": [
    "# 1. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7082881e-2055-47ac-8ac6-dfa2d3944395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "df = pd.read_csv('./data/hackathon_train.csv', encoding='cp949', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdbe7f1-f09a-4213-8a1a-a2d1fd8b493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 지냅니다.\n",
      "<중립>  다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며 요즘은 새에 관심이 조금 있습니다. 흥미가 있어도 관심도가 오래가진 않아 중립으로 하였습니다.\n",
      "<그렇다> 감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 영화관도 잘 가지 않습니다. 머리로는 안 울고 싶은데 이미 울고 있어서 당황스러울 때가 많습니다.\n",
      "<중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고 변수가 생길 때 세워둔 대책이 없으면 머릿속이 하얘져서 대처하기 힘들기 때문입니다. 하지만 너무 많은 변수를 생각하다가 귀찮아져서 대비책을 세우는 것을 중간에 그만둡니다.\n",
      "<아니다> 평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대로 못합니다. 면접을 볼 때 화상 면접이었는데도 스스로 더한 압박감을 느껴 연습할 때 보다 훨씬 말하지 못했습니다.\n",
      "<그렇다> 저는 파티나 행사에 간 적이 거의 없으며 간다고 가정을 하면 알고 있는 사람과 대화하는 편입니다. 이유는 모르는 사람에게 자신을 소개하는 것이 부담스럽습니다.\n",
      "<아니다> 저는 여러 가지 프로젝트를 문어발식으로 조금씩 시작하여 진행이 잘 되는 프로젝트를 먼저 끝내는 방식으로 일을 합니다. 학교 다닐 때 과제가 여러 개가 있으면 하나를 하다가 집중력이 떨어지거나 막히는 일이 있을 때 다른 과제로 옮겨가는 식으로 하였습니다.\n",
      "<그렇다> 매우 감상적인 편입니다. 책이나 드라마 영화 등 매체를 볼 때 무언가 다치거나 아프면 굉장히 슬퍼하고 우울해합니다.\n",
      "<중립> 계획 세우는 일을 좋아하진 않지만, 여행 계획은 시간 단위로 세세하게 세웁니다. 많이 나가지 않다 보니 그 여행지에 어떤 것이 있는지 놓치고 싶지 않고 더 알차게 보내고 싶어서입니다. 다이어리는 사지만 쓰다가 안 쓰는 것을 반복하는 편입니다.\n",
      "<중립> 가끔 집안 가게에서 매대를 보는데 계산 실수를 할 때가 있습니다. 그럴 때면 그럴 수도 있지 되뇌면서도 그 실수한 것이 머릿속에서 떠나질 않습니다.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df['Answer'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2bb230-12e4-4f04-8e10-2c78cb912c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_hanbert import HanBertTokenizer\n",
    "# load pretrained model\n",
    "# model_name = 'kykim/bert-kor-base'\n",
    "# model_name = 'monologg/kobigbird-bert-base'\n",
    "# model_name = 'beomi/kcbert-base'\n",
    "model_name = 'snunlp/KR-BERT-char16424'\n",
    "# model_name = 'kykim/albert-kor-base'\n",
    "# model_name = 'HanBert-54kN-torch'\n",
    "# model_name = 'Twitter/twhin-bert-base'\n",
    "# model_name = 'klue/roberta-large'\n",
    "\n",
    "# model_name = 'klue/bert-base'\n",
    "# model_name = 'klue/roberta-base'\n",
    "# model_name = 'skt/kogpt2-base-v2'\n",
    "def get_model(model_name, embed_dim):\n",
    "    # * Model          | Tokenizer          | Pretrained weights shortcut\n",
    "    # MODEL=(DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "    # tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(\"skt/kogpt2\")\n",
    "    # model = GPT2Model.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "    # tokenizer = HanBertTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # model = BertModel.from_pretrained(model_name)\n",
    "    # use this one for albert\n",
    "    # model = AlbertModel.from_pretrained(model_name)\n",
    "    # model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "    # n_hl = model.config.num_hidden_layers\n",
    "    n_hl = 20\n",
    "    # embed_dim = model.config.embedding_size\n",
    "    \n",
    "    return model, tokenizer, n_hl, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e967cc0e-7418-464a-8917-29b59e07412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"snunlp/KR-BERT-char16424\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16424\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--snunlp--KR-BERT-char16424/snapshots/47521960ac7595c5d2ed643f7a9dab9b0efcf58d/pytorch_model.bin\n",
      "Some weights of the model checkpoint at snunlp/KR-BERT-char16424 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at snunlp/KR-BERT-char16424.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, n_hl, embed_dim = get_model(model_name, 768)\n",
    "# model_korean, tokenizer_korean, n_hl_korean, embed_dim_korean = get_model(model_name, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4307b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(16424, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d5f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mbti_to_label(mbti: str):\n",
    "    \"\"\"\n",
    "    :param mbti: string. length=4\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    stand = 'ISTJ'  # [0, 0, 0, 0]\n",
    "    result = []\n",
    "    for i in range(4):\n",
    "        if stand[i] == mbti[i]:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "\n",
    "    return result\n",
    "# stand = 'ISTJ'\n",
    "# label = df['MBTI'].map(convert_mbti_to_label)\n",
    "# for i in range(4):\n",
    "#     df['is_{}'.format(stand[i])] = label.map(lambda x: x[i])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22dff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now let's see the correlation between each mbti columns and \"Gender\" as well as \"Age\"\n",
    "# # let's utilize \"groupby\" function\n",
    "# for col_name in ['Gender', 'Age']:    \n",
    "#     for label in stand:\n",
    "#         print('correlation between {} and {}'.format(col_name, label))\n",
    "#         print(df.groupby(f'is_{label}')[f'{col_name}'].describe())\n",
    "#         print('correlation value: ', df[f'is_{label}'].corr(df[f'{col_name}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80211e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Age_norma'] = df['Age'].map(lambda x: (x - df['Age'].mean()) / df['Age'].std())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea92b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # map the gender to '남\n",
    "# df['Gender_korean'] = df['Gender'].apply(lambda x: '남자' if x == 0 else '여자')\n",
    "# # df['Age_enc'] = df['Age'].apply(lambda x: 0 if x == 10 else 1 if x == 20 else 2 if x == 30 else 3 if x == 40 else 4)\n",
    "# df['Age_korean'] = df['Age'].apply(lambda x: '30' if x == 30 else '40' if x == 40 else '20')\n",
    "# # df['Age_korean'] = df['Age'].apply(lambda x: '서른 살 (30)' if x == 30 else '마흔 살 (40)' if x == 40 else '스무 살 (20)')\n",
    "\n",
    "# df['One_word_ans'] = df['Answer'].apply(lambda x: (x.split('>')[0])[x.index('<')+1:])\n",
    "# # df['One_word_ans'] = df['One_word_ans'].astype('category').cat.codes\n",
    "# df['One_word_ans'].unique()\n",
    "\n",
    "df['Answer'] = df['Answer'].apply(lambda x: x.replace(f\"<{(x.split('>')[0])[x.index('<')+1:]}>\", ''))\n",
    "\n",
    "df_question = pd.read_excel('./data/Question.xlsx', sheet_name='Sheet1', index_col=0)\n",
    "df_question['Question']\n",
    "df['Question'] = ''\n",
    "for i, row in df.iterrows():\n",
    "    df.loc[i, 'Question'] = df_question.loc[row['Q_number'], 'Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0208124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate 'Gender_korean', 'Age_korean', 'One_word_ans' with ' ' (space)\n",
    "# # df['concatenated_korean'] = '질문:' + df['Question'] + '_정답:' + df['Answer']\n",
    "# df['concatenated_korean'] = df['Age_korean'] + '세의 ' + df['Gender_korean'] + '는 ' + df['Question'] + '질문에 ' + df['Answer'] + '라는 대답을 했습니다'\n",
    "# # df['concatenated_korean'] = \n",
    "# #df['concatenated_korean'] = df['Answer'] ----- only ans\n",
    "# # '30세의 남자는 {Q} 질문에 {A}라는 대답을 했습니다.'\n",
    "# # df['Gender_korean'] + '_' + df['Age_korean'] + '_질문:' + df['Question'] + '_단답:' + df['One_word_ans'] + '_정답:' + df['Answer']\n",
    "\n",
    "# df['concatenated_korean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d863bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# import pandas as pd\n",
    "# Replace this with your DataFrame\n",
    "# df = pd.read_csv('./data/hackathon_train.csv', encoding='cp949', index_col=0)\n",
    "\n",
    "\n",
    "def swap_words(text, word_list):\n",
    "    words = text.split()\n",
    "    if len(words) > 1:\n",
    "        idx = random.randrange(len(words) - 1)\n",
    "        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        # print(f'{words[idx]} <-> {words[idx+1]}')\n",
    "    return ' '.join(words)\n",
    "\n",
    "def insert_word(text, word_list):\n",
    "    words = text.split()\n",
    "    idx = random.randrange(len(words) + 1)\n",
    "    new_word = random.choice(word_list)\n",
    "    words.insert(idx, new_word)\n",
    "    # print(f'inserted {new_word} at index {idx}')\n",
    "    return ' '.join(words)\n",
    "\n",
    "def delete_word(text, word_list):\n",
    "    words = text.split()\n",
    "    if len(words) > 1:\n",
    "        idx = random.randrange(len(words))\n",
    "        # print(f'deleted {words[idx]} at index {idx}')\n",
    "        del words[idx]\n",
    "        \n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment_operation(df, prob, operation, row_name='Answer', word_list=None):\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if random.random() < prob:\n",
    "            new_row = row.copy()\n",
    "            new_row[row_name] = operation(row[row_name], word_list)\n",
    "            new_rows.append(new_row)\n",
    "    return pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "# Apply the augmentations\n",
    "def augment(df, word_list, prob_syn=0.1, prob_swap=0.1, prob_ins=0.0, prob_del=0.0, row_name='Answer'):\n",
    "    \n",
    "    df_augmented = df.copy()\n",
    "    \n",
    "    # Swap words\n",
    "    df_augmented = augment_operation(df_augmented, prob_swap, swap_words, row_name)\n",
    "    \n",
    "    # Insert words\n",
    "    df_augmented = augment_operation(df_augmented, prob_ins, insert_word, row_name, word_list)\n",
    "    \n",
    "    # Delete words\n",
    "    df_augmented = augment_operation(df_augmented, prob_del, delete_word, row_name)\n",
    "    \n",
    "    # generate synonyms\n",
    "    df_augmented = augment_operation(df_augmented, prob_syn, generate_synonym, row_name)\n",
    "    \n",
    "    print(f'original size: {df.shape}')\n",
    "    # print(df)\n",
    "    print(f'augmented size: {df_augmented.shape}')\n",
    "\n",
    "    return df_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0285fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to insert (for the insert_word function)\n",
    "word_list = ['제 생각에는', '사실 그것은']\n",
    "# df_augmented = augment(df, prob, word_list, swap_words=True, insert_words=True, delete_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180fbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_augmented.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aba372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from hanspell import spell_checker\n",
    "import json\n",
    "from num2words import num2words\n",
    "from kss import split_sentences\n",
    "\n",
    "def sentence_segmentation(text):\n",
    "    sentences = split_sentences(text)\n",
    "    return sentences\n",
    "\n",
    "def pos_tagging(text):\n",
    "    okt = Okt()\n",
    "    tagged = okt.pos(text)\n",
    "    return tagged\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text, apply_ngrams=False, ngram_size=1, apply_replacements=True, apply_normalization=True, remove_stopwords=False):\n",
    "    # Check and correct spelling\n",
    "    # checked_spelling = spell_checker.check(text)\n",
    "    # text = checked_spelling.checked\n",
    "\n",
    "    # Remove special characters using regular expressions\n",
    "    # text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s]\", \" \", text)\n",
    "\n",
    "    # Initialize the Okt tokenizer\n",
    "    okt = Okt()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    # words = okt.morphs(text, stem=False)\n",
    "    words = text.split()\n",
    "    # print(words)\n",
    "\n",
    "    # if apply_ner:\n",
    "    #     # Perform named entity recognition\n",
    "    #     entities = okt.nouns(text)\n",
    "    #     words = [\"[NER]\" if word in entities else word for word in words]\n",
    "\n",
    "    if apply_replacements:\n",
    "        # Replace specific words or phrases (example)\n",
    "        replacements = {\"아니다\": \"아닙니다\", \"그렇다\": \"그렇습니다\", \"이다\": \"입니다\"}\n",
    "        words = [replacements.get(word, word) for word in words]\n",
    "\n",
    "    if apply_normalization:\n",
    "        # Perform text normalization \n",
    "        words = [num2words(word) if word.isdigit() else word for word in words]\n",
    "        words = [okt.normalize(word) for word in words]\n",
    "        # words = [normalization_map.get(word, word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '다']\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "\n",
    "    # Remove short and long words\n",
    "    # words = [word for word in words if 3 <= len(word) <= 30]\n",
    "\n",
    "    if apply_ngrams:\n",
    "        # Generate N-grams\n",
    "        ngrams = zip(*[words[i:] for i in range(ngram_size)])\n",
    "        ngrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "        words.extend(ngrams)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbcfa3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 저는 파티나 행사에 간 적이 거의 없으며 간다고 가정을 하면 알고 있는 사람과 대화하는 편입니다. 이유는 모르는 사람에게 자신을 소개하는 것이 부담스럽습니다.\n",
      "저는 파티나 행사에 간 적이 거의 없으며 간다고 가정을 하면 알고 있는 사람과 대화하는 편입니다. 이유는 모르는 사람에게 자신을 소개하는 것이 부담스럽습니다. 저는 파티나 파티나 행사에 행사에 간 간 적이 적이 거의 거의 없으며 없으며 간다고 간다고 가정을 가정을 하면 하면 알고 알고 있는 있는 사람과 사람과 대화하는 대화하는 편입니다. 편입니다. 이유는 이유는 모르는 모르는 사람에게 사람에게 자신을 자신을 소개하는 소개하는 것이 것이 부담스럽습니다.\n"
     ]
    }
   ],
   "source": [
    "# example to see the result of preprocessing\n",
    "ans = df['Answer'].iloc[5]\n",
    "print(ans)\n",
    "\n",
    "print(preprocess_text(ans, apply_ngrams=True, ngram_size=2, apply_replacements=True, apply_normalization=True, remove_stopwords=False))\n",
    "# print(pos_tagging(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc2b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_augmented.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b06878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('저는', 1), ('파티나', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "processed_ans = preprocess_text(ans, apply_ngrams=False, ngram_size=1, apply_replacements=True, apply_normalization=True)\n",
    "preprocessed_ans = processed_ans.split(' ')\n",
    "# print(preprocessed_ans)\n",
    "cnt = Counter(preprocessed_ans)\n",
    "cnt.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66931cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now preprocess the entire 'Answer' column\n",
    "df['preprocessed_ans'] = df['Answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e584f2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>MBTI</th>\n",
       "      <th>Q_number</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question</th>\n",
       "      <th>preprocessed_ans</th>\n",
       "      <th>labels</th>\n",
       "      <th>is_I</th>\n",
       "      <th>is_S</th>\n",
       "      <th>is_T</th>\n",
       "      <th>is_J</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>INFP</td>\n",
       "      <td>1</td>\n",
       "      <td>어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만...</td>\n",
       "      <td>주기적으로 새로운 친구를 만드나요? 경험을 비추어봤을 때 어떤지와 그러한 이유가 궁...</td>\n",
       "      <td>어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 ...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>INFP</td>\n",
       "      <td>2</td>\n",
       "      <td>다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며 요즘...</td>\n",
       "      <td>자유 시간 중 상당 부분을 다양한 관심사를 탐구하는 데 할애하나요? 요즘 어떤 관심...</td>\n",
       "      <td>다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며 요즘은 ...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>INFP</td>\n",
       "      <td>3</td>\n",
       "      <td>감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 영화관도 ...</td>\n",
       "      <td>다른 사람이 울고 있는 모습을 보면 자신도 울고 싶어질 때가 많나요? 이런 상황에서...</td>\n",
       "      <td>감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 영화관도 잘...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>INFP</td>\n",
       "      <td>4</td>\n",
       "      <td>대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고 변수가...</td>\n",
       "      <td>일이 잘못될 때를 대비해 여러 대비책을 세우는 편인가요? 이유는 무엇인가요.</td>\n",
       "      <td>대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고 변수가 ...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>INFP</td>\n",
       "      <td>5</td>\n",
       "      <td>평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대로 못합니...</td>\n",
       "      <td>압박감이 심한 환경에서도 평정심을 유지하는 편인가요? 최근 경험을 말씀해주세요.</td>\n",
       "      <td>평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대로 못합니다...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14384</th>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>44</td>\n",
       "      <td>저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획으로 인한...</td>\n",
       "      <td>계획에 차질이 생기면 최대한 빨리 계획으로 돌아가기 위해 노력하나요? 최근에 있었던...</td>\n",
       "      <td>저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획으로 인한 ...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14385</th>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>45</td>\n",
       "      <td>저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적도 많기 ...</td>\n",
       "      <td>오래전의 실수를 후회할 때가 많나요? 요즘에 무엇때문에 그런가요.</td>\n",
       "      <td>저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적도 많기 때...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>46</td>\n",
       "      <td>저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이미 정해진...</td>\n",
       "      <td>인간의 존재와 삶의 이유에 대해 깊이 생각하지 않는 편인가요? 답변에 대한 이유를 ...</td>\n",
       "      <td>저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이미 정해진 ...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>47</td>\n",
       "      <td>저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하지 않으려...</td>\n",
       "      <td>감정을 조절하기보다는 감정에 휘둘리는 편인가요? 본인의 생각과 주변의 평가는 어떤가요.</td>\n",
       "      <td>저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하지 않으려 ...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14388</th>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>48</td>\n",
       "      <td>저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다. 이유는 ...</td>\n",
       "      <td>상대방의 잘못이라는 것이 명백할 때도 상대방의 체면을 살려주기 위해 노력하나요? 이...</td>\n",
       "      <td>저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다. 이유는 잘...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11520 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_ID  Gender  Age  MBTI  Q_number  \\\n",
       "Data_ID                                         \n",
       "1              1       1   30  INFP         1   \n",
       "2              1       1   30  INFP         2   \n",
       "3              1       1   30  INFP         3   \n",
       "4              1       1   30  INFP         4   \n",
       "5              1       1   30  INFP         5   \n",
       "...          ...     ...  ...   ...       ...   \n",
       "14384        240       0   40  ISTJ        44   \n",
       "14385        240       0   40  ISTJ        45   \n",
       "14386        240       0   40  ISTJ        46   \n",
       "14387        240       0   40  ISTJ        47   \n",
       "14388        240       0   40  ISTJ        48   \n",
       "\n",
       "                                                    Answer  \\\n",
       "Data_ID                                                      \n",
       "1         어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만...   \n",
       "2          다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며 요즘...   \n",
       "3         감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 영화관도 ...   \n",
       "4         대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고 변수가...   \n",
       "5         평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대로 못합니...   \n",
       "...                                                    ...   \n",
       "14384     저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획으로 인한...   \n",
       "14385     저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적도 많기 ...   \n",
       "14386     저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이미 정해진...   \n",
       "14387     저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하지 않으려...   \n",
       "14388     저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다. 이유는 ...   \n",
       "\n",
       "                                                  Question  \\\n",
       "Data_ID                                                      \n",
       "1        주기적으로 새로운 친구를 만드나요? 경험을 비추어봤을 때 어떤지와 그러한 이유가 궁...   \n",
       "2        자유 시간 중 상당 부분을 다양한 관심사를 탐구하는 데 할애하나요? 요즘 어떤 관심...   \n",
       "3        다른 사람이 울고 있는 모습을 보면 자신도 울고 싶어질 때가 많나요? 이런 상황에서...   \n",
       "4               일이 잘못될 때를 대비해 여러 대비책을 세우는 편인가요? 이유는 무엇인가요.   \n",
       "5             압박감이 심한 환경에서도 평정심을 유지하는 편인가요? 최근 경험을 말씀해주세요.   \n",
       "...                                                    ...   \n",
       "14384    계획에 차질이 생기면 최대한 빨리 계획으로 돌아가기 위해 노력하나요? 최근에 있었던...   \n",
       "14385                 오래전의 실수를 후회할 때가 많나요? 요즘에 무엇때문에 그런가요.   \n",
       "14386    인간의 존재와 삶의 이유에 대해 깊이 생각하지 않는 편인가요? 답변에 대한 이유를 ...   \n",
       "14387     감정을 조절하기보다는 감정에 휘둘리는 편인가요? 본인의 생각과 주변의 평가는 어떤가요.   \n",
       "14388    상대방의 잘못이라는 것이 명백할 때도 상대방의 체면을 살려주기 위해 노력하나요? 이...   \n",
       "\n",
       "                                          preprocessed_ans        labels  \\\n",
       "Data_ID                                                                    \n",
       "1        어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 ...  [0, 1, 1, 1]   \n",
       "2        다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며 요즘은 ...  [0, 1, 1, 1]   \n",
       "3        감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 영화관도 잘...  [0, 1, 1, 1]   \n",
       "4        대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고 변수가 ...  [0, 1, 1, 1]   \n",
       "5        평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대로 못합니다...  [0, 1, 1, 1]   \n",
       "...                                                    ...           ...   \n",
       "14384    저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획으로 인한 ...  [0, 0, 0, 0]   \n",
       "14385    저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적도 많기 때...  [0, 0, 0, 0]   \n",
       "14386    저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이미 정해진 ...  [0, 0, 0, 0]   \n",
       "14387    저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하지 않으려 ...  [0, 0, 0, 0]   \n",
       "14388    저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다. 이유는 잘...  [0, 0, 0, 0]   \n",
       "\n",
       "         is_I  is_S  is_T  is_J  \n",
       "Data_ID                          \n",
       "1           0     1     1     1  \n",
       "2           0     1     1     1  \n",
       "3           0     1     1     1  \n",
       "4           0     1     1     1  \n",
       "5           0     1     1     1  \n",
       "...       ...   ...   ...   ...  \n",
       "14384       0     0     0     0  \n",
       "14385       0     0     0     0  \n",
       "14386       0     0     0     0  \n",
       "14387       0     0     0     0  \n",
       "14388       0     0     0     0  \n",
       "\n",
       "[11520 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand = 'ISTJ'\n",
    "df['labels'] = df['MBTI'].map(convert_mbti_to_label)\n",
    "for i in range(4):\n",
    "    df['is_{}'.format(stand[i])] = df['labels'].map(lambda x: x[i])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5160116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test dataframe\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "for idx in df['User_ID'].unique():\n",
    "    train_df_list.append(df[df['User_ID']==idx][0:40])\n",
    "    test_df_list.append(df[df['User_ID']==idx][40:])\n",
    "\n",
    "train_df = pd.concat(train_df_list, ignore_index=True)\n",
    "test_df = pd.concat(test_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ad5324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: (9600, 13)\n",
      "augmented size: (12778, 13)\n"
     ]
    }
   ],
   "source": [
    "# augment the train dataframe\n",
    "prob_syn, prob_swap, prob_ins, prob_del = 0.0, 0.1, 0.1, 0.1\n",
    "train_df_augmented = augment(train_df, word_list, prob_syn=prob_syn, prob_swap=prob_swap, prob_ins=prob_ins, prob_del=prob_del, row_name='preprocessed_ans')\n",
    "train_df = train_df_augmented.copy()\n",
    "# test_df_augmented = augment(test_df, word_list, prob_syn=prob_syn, prob_swap=prob_swap, prob_ins=prob_ins, prob_del=prob_del, row_name='preprocessed_ans')\n",
    "# test_df = test_df_augmented.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1788357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's add number of words as a feature\n",
    "# train_df['num_words'] = train_df['preprocessed_ans'].apply(lambda x: len(x.split()))\n",
    "# test_df['num_words'] = test_df['preprocessed_ans'].apply(lambda x: len(x.split()))\n",
    "# train_df['num_sentences'] = train_df['preprocessed_ans'].apply(lambda x: len(sentence_segmentation(x)))\n",
    "# test_df['num_sentences'] = test_df['preprocessed_ans'].apply(lambda x: len(sentence_segmentation(x)))\n",
    "\n",
    "# # let's compare the number of words between introverts and extroverts\n",
    "# train_df['num_words_per_sentence'] = train_df['num_words'] / train_df['num_sentences']\n",
    "# test_df['num_words_per_sentence'] = test_df['num_words'] / test_df['num_sentences']\n",
    "# train_df['num_words_per_sentence'].groupby(train_df['is_J']).describe()\n",
    "# test_df['num_words_per_sentence'].groupby(test_df['is_J']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54827f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's see more the introverts' answers\n",
    "# words_dict = {} # count the frequency of each words\n",
    "\n",
    "# intro_df = df[df['is_S'] == 1] # change the label to see the other mbti types\n",
    "\n",
    "# for i, row in intro_df.iterrows():\n",
    "#     words = preprocess_text(row['Answer']).split()\n",
    "#     for word in words:\n",
    "#         if word in words_dict:\n",
    "#             words_dict[word] += 1\n",
    "#         else:\n",
    "#             words_dict[word] = 1\n",
    "# # wrap the words_dict into 'Counter' class\n",
    "# cnt_words = Counter(words_dict)\n",
    "# cnt_words.most_common(30) # we will decide the frequency of which words to be tracked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79abc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extro_cnt_words = {}\n",
    "# extro_df = df[df['is_S'] == 0]\n",
    "\n",
    "# for i, row in extro_df.iterrows():\n",
    "#     words = preprocess_text(row['Answer']).split()\n",
    "#     for word in words:\n",
    "#         if word in extro_cnt_words:\n",
    "#             extro_cnt_words[word] += 1\n",
    "#         else:\n",
    "#             extro_cnt_words[word] = 1\n",
    "\n",
    "# extro_cnt_words = Counter(extro_cnt_words)\n",
    "# extro_cnt_words.most_common(30) # we will decide the frequency of which words to be tracked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de7c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find words which are more frequently used by introverts than extroverts\n",
    "# common_extro_cnt_words = [word for word, _ in extro_cnt_words.most_common(30)]\n",
    "# common_cnt_words = [word for word, _ in cnt_words.most_common(30)]\n",
    "\n",
    "# only_intro_words = [word for word in common_cnt_words if word not in common_extro_cnt_words]\n",
    "# only_extro_words = [word for word in common_extro_cnt_words if word not in common_cnt_words]\n",
    "# # len(only_intro_words), len(only_extro_words)\n",
    "# only_extro_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e310e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Assuming your DataFrame is named `df` and has a column named \"Answer\"\n",
    "# # containing the preprocessed text data.\n",
    "\n",
    "# # Initialize the TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Calculate the TF-IDF values for the \"Answer\" column\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['Answer'].apply(preprocess_text).values)\n",
    "\n",
    "# # You can convert the TF-IDF matrix to a DataFrame if needed\n",
    "# tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# # Now, `tfidf_df` contains the calculated TF-IDF values for each word in each document.\n",
    "# tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1729d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the column name of the max value in each row\n",
    "# tfdf['freq_word'] = tfidf_df.idxmax(axis=1)\n",
    "# df['freq_word']\n",
    "# find the number of 'Nan's in the column named 'freq_word'\n",
    "\n",
    "# num_nans = df['freq_word']\n",
    "# tfidf_df['freq_word'] = tfidf_df.idxmax(axis=1)\n",
    "# df['freq_word'] = tfidf_df['freq_word']\n",
    "# df['freq_word'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d8ed8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12778, 13), (1920, 13))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # set a pad token if you are using GPT2\n",
    "# # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# # be careful to comment this only if you tokenize the \"Answer\" column \n",
    "# train_df['Answer_rep'] = train_df['Answer'].apply(lambda x: x.replace(f\"<{(x.split('>')[0])[x.index('<')+1:]}>\", ''))\n",
    "# test_df['Answer_rep'] = test_df['Answer'].apply(lambda x: x.replace(f\"<{(x.split('>')[0])[x.index('<')+1:]}>\", ''))\n",
    "# train_df[\"preprocessed_answer\"] = train_df[\"Answer_rep\"].apply(preprocess_text)\n",
    "# test_df[\"preprocessed_answer\"] = test_df[\"Answer_rep\"].apply(preprocess_text)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bed86fe-d50b-49a3-aad6-89a28719d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are eliminating the one-word-short answer -> please modify Answer to Answer_rep\n",
    "\n",
    "train_tensor = tokenizer(train_df['preprocessed_ans'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True, truncation=True)\n",
    "test_tensor = tokenizer(test_df['preprocessed_ans'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True, truncation=True)\n",
    "# train_tensor = tokenizer(train_df['concatenated_korean'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True)\n",
    "# test_tensor = tokenizer(test_df['concatenated_korean'].to_list(), max_length=model.config.max_position_embeddings, return_tensors='pt', padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "589ad9a4-8b31-4249-a9e1-05a49fbeeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMapDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {k:v[idx] for k,v in self.data.items()}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b90b819c-5fdb-4778-84d0-523d26732353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(tokens, batch_size=32, shuffle=False):\n",
    "    ds = MyMapDataset(tokens)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "# train_ds = MyMapDataset(train_tensor)\n",
    "# test_ds = MyMapDataset(test_tensor)\n",
    "\n",
    "train_tensor_dl = build_dataloader(train_tensor, batch_size=32, shuffle=False)\n",
    "test_tensor_dl = build_dataloader(test_tensor, batch_size=32, shuffle=False)\n",
    "# train_tensor_korean_dl = build_dataloader(train_tensor_korean, batch_size=32, shuffle=False)\n",
    "# test_tensor_korean_dl = build_dataloader(test_tensor_korean, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d55d08bd-4f8e-4f6a-87ff-c305309e3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, dl, device=0):\n",
    "    pooled = []\n",
    "    hidden = []\n",
    "    model.cuda(device)\n",
    "    model.eval()\n",
    "    for data in dl:\n",
    "        data = {k:v.cuda(device) for k,v in data.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**data, output_hidden_states=True)\n",
    "            # input_ids = data['input_ids']\n",
    "            # output = model(**data, return_dict=True)\n",
    "        p, h = output.pooler_output, output.hidden_states\n",
    "        # p, h = output.last_hidden_state[:,0,:], output.last_hidden_state # only [CLS] token embedding\n",
    "        # p = output.hidden_states[-1][:, -1, :]\n",
    "        pooled.append(p) # pooler output\n",
    "        hidden.append(h[-1][:,0,:]) # only [CLS] token embedding \n",
    "        # hidden.append(h) # all token embedding\n",
    "    return torch.cat(pooled), torch.cat(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "361e9997-879e-4c49-a51f-e2ebbb9bdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = forward(model, train_tensor_dl, device=0)\n",
    "test_result = forward(model, test_tensor_dl, device=1)\n",
    "# train_result_korean = forward(model, train_tensor_korean_dl, device=0)\n",
    "# test_result_korean = forward(model, test_tensor_korean_dl, device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93bc8d80-78f9-4dc5-9e94-d5e4203bd200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12778, 768]), torch.Size([1920, 768]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result[0].shape, test_result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "febc6d40-e4eb-458d-b6db-f7c781a71178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12778, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24dda5b7-f63d-4526-9083-1851c2acba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_result, f'{model_name.replace(\"/\", \"_\")}_train_preprocessed.pt')\n",
    "# torch.save(test_result, f'{model_name.replace(\"/\", \"_\")}_test_preprocessed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06ac51c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12778, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec4092a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result, test_result = train_result[0], test_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5697a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12778, 76)\n",
      "torch.Size([12778, 768])\n"
     ]
    }
   ],
   "source": [
    "# added_cols = ['Gender', 'Age'] + [col for col in train_df.columns if col.startswith('is')]\n",
    "train_df = pd.get_dummies(data=train_df, columns=['Age'], prefix='cat')\n",
    "test_df = pd.get_dummies(data=test_df, columns=['Age'], prefix='cat')\n",
    "added_cols = ['Gender'] + [col for col in train_df.columns if col.startswith('cat')]\n",
    "nums = 48\n",
    "# for i in range(nums):\n",
    "#     for col in added_cols:\n",
    "#         if col.startswith('cat'):\n",
    "#             if i % 6 == 0:\n",
    "#                 added_cols.append(col)\n",
    "#         else:\n",
    "#             added_cols.append(col)\n",
    "\n",
    "for _ in range(nums):\n",
    "    added_cols += ['Gender']\n",
    "\n",
    "for _ in range(nums//6):\n",
    "    # added_cols += ['num_sentences', 'num_words_per_sentence']\n",
    "    added_cols += [col for col in train_df.columns if col.startswith('cat')]\n",
    "\n",
    "\n",
    "col_data = train_df[added_cols].values\n",
    "test_col_data = test_df[added_cols].values\n",
    "# print(col_data[0][0].shape)\n",
    "col_data = col_data.astype(float)\n",
    "test_col_data = test_col_data.astype(float)\n",
    "print(col_data.shape)\n",
    "print(train_result.shape)\n",
    "\n",
    "col_data = torch.tensor(col_data, dtype=torch.float, device='cuda:0')\n",
    "test_col_data = torch.tensor(test_col_data, dtype=torch.float, device='cuda:1')\n",
    "\n",
    "# train_result_added = torch.cat([train_result, col_data], dim=1)\n",
    "# test_result_added = torch.cat([test_result, test_col_data], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20ba1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_result_added.shape, test_result_added.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c71fe611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random(SEED=0):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label, label_idx=0):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.label_idx = label_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], torch.tensor(self.label[idx][self.label_idx])\n",
    "    \n",
    "def convert_mbti_to_label(mbti: str):\n",
    "    \"\"\"\n",
    "    :param mbti: string. length=4\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    stand = 'ISTJ'  # [0, 0, 0, 0]\n",
    "    result = []\n",
    "    for i in range(4):\n",
    "        if stand[i] == mbti[i]:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# MBTI = ['IE', 'SN', 'TF', 'JP']\n",
    "# # stand = 'ISTJ'\n",
    "# label = df['MBTI'].map(convert_mbti_to_label)\n",
    "# for i in range(4):\n",
    "#     df['is_{}'.format(MBTI[i])] = label.map(lambda x: x[i])\n",
    "# df = df.drop(columns=['is_IE'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbcbbc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/cudf/utils/utils.py:218: FutureWarning: The cudf.set_allocator function is deprecated and will be removed in a future release. Please use rmm.reinitialize (https://docs.rapids.ai/api/rmm/stable/api.html#rmm.reinitialize) instead. Note that `cudf.set_allocator(allocator=\"managed\")` is equivalent to `rmm.reinitialize(managed_memory=True)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from torch.optim import Adam\n",
    "# from thundersvm import SVC as ThunderSVC\n",
    "import cudf\n",
    "from cuml.svm import SVC\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "gpu_id = 0\n",
    "cudf.set_allocator(\"managed\")\n",
    "cudf.cuda.select_device(gpu_id)\n",
    "def main(label_idx, epochs, name, test_number, col_data, test_col_data, train_label_1d, test_label_1d, train_result, test_result):\n",
    "    print(f'{col_data.shape} <- col_data, {train_result.shape} <- train_result')\n",
    "    \n",
    "    mlp = nn.Sequential(nn.Linear(col_data.shape[1], 786),\n",
    "                        #   nn.Dropout(0.35),\n",
    "                          nn.ReLU(),        \n",
    "                          nn.Linear(786, 3886),\n",
    "                        #   nn.ReLU(),\n",
    "                        #   nn.Linear(786//2, 1086),\n",
    "                        #   nn.ReLU(),\n",
    "                        #   nn.Linear(786, 786),\n",
    "                          )\n",
    "\n",
    "    # if chkpt_path is not None:\n",
    "    #     mlp.load_state_dict(torch.load(chkpt_path))\n",
    "    \n",
    "    mlp = mlp.cuda(0)\n",
    "    # Define the optimizer\n",
    "    optimizer = Adam(mlp.parameters(), lr=1e-2)\n",
    "\n",
    "    # input to the MLP -> col_data (they can be on cuda:0) -> output -> 768\n",
    "    # then concatenate col_data and train_result[0] to give train_result_added\n",
    "    # and feed the concatenated tensor to the svm\n",
    "    \n",
    "    # Training\n",
    "    svm_model = SVC(kernel='rbf', C=1.5, gamma=0.05, probability=True)\n",
    "    # if svm_path is not None:\n",
    "    #     svm_model = joblib.load(svm_path)\n",
    "    # test_col_data = test_col_data.cuda(0)\n",
    "    \n",
    "    # test_col_data = test_col_data.cuda(0)\n",
    "    \n",
    "    # svm_model = SVC(kernel='rbf', C=1, gamma=0.085)\n",
    "    # hinge_losses = []\n",
    "    # min_hinge_loss = np.inf\n",
    "    max_auc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # if mlp is in cuda:1 move it to cuda:0\n",
    "        \n",
    "        if mlp[0].weight.device != torch.device('cuda:0'):\n",
    "            mlp = mlp.cuda(0)\n",
    "        # Forward pass through the MLP\n",
    "        # mlp_output_train = mlp(col_data) # output has shape of (9600, 768)\n",
    "        mlp_output_train = mlp(col_data.to(mlp[0].weight.dtype))\n",
    "\n",
    "        # print(f'passing {col_data} to the mlp')\n",
    "\n",
    "        # Concatenate the output of the MLP with the additional features\n",
    "        concatenated_features_train = np.hstack([mlp_output_train.detach().cpu().numpy(), train_result.detach().cpu().numpy()]) # shape of (9600, 768 + 768)\n",
    "        # print(f'concatenating mlp output with the embedded features')\n",
    "\n",
    "        # Train the SVM\n",
    "        svm_model.fit(concatenated_features_train, train_label_1d) # shape of (9600, 768 + 768)\n",
    "\n",
    "        # Compute the hinge loss\n",
    "        hinge_loss = torch.nn.functional.multi_margin_loss(mlp_output_train, torch.from_numpy(train_label_1d).cuda(0), margin=1.0)\n",
    "\n",
    "        \n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        hinge_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        with torch.no_grad():\n",
    "            # first move the mlps to the cuda:1\n",
    "\n",
    "            # mlp = mlp.cuda(1)\n",
    "            test_col_data = test_col_data.cuda(0)\n",
    "            test_col_data = test_col_data.to(mlp[0].weight.dtype)\n",
    "            mlp_output_test = mlp(test_col_data)\n",
    "            concatenated_features_test = np.hstack([mlp_output_test.detach().cpu().numpy(), test_result.detach().cpu().numpy()])\n",
    "            accuracy = svm_model.score(concatenated_features_test, test_label_1d)\n",
    "            # test_prob_pred = svm_model.predict_proba(concatenated_features_test)\n",
    "            prob_preds = svm_model.predict_proba(concatenated_features_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(test_label_1d, prob_preds)\n",
    "            # training_accuracy = svm_model.score(concatenated_features_train, train_label_1d)\n",
    "            \n",
    "            \n",
    "            if roc_auc > max_auc:\n",
    "                max_auc = roc_auc\n",
    "                torch.save(mlp.state_dict(), f'./{name}_model_{test_number}.pth')\n",
    "                # svm_model.save_to_file(f'./{name}_svm_{test_number}.pkl')\n",
    "                # also save it using joblib\n",
    "                joblib.dump(svm_model, f'./{name}_svm_{test_number}.pkl')\n",
    "            \n",
    "            # print(f'saving the models with the minimum hinge loss: {min_hinge_loss}')\n",
    "            \n",
    "        if (epoch+1) % 5 == 0:    \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Hinge Loss: {hinge_loss.item()}, Train_acc: , Accuracy: {accuracy}, ROC AUC: {roc_auc}')\n",
    "        \n",
    "    # if it is the last epoch, save the predictions and add it in the next training data\n",
    "        \n",
    "    train_prob_preds = svm_model.predict_proba(concatenated_features_train)[:, 1]\n",
    "        \n",
    "    return max_auc, accuracy, train_prob_preds, prob_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "131231be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12778, 76]) <- col_data, torch.Size([12778, 768]) <- train_result\n",
      "Epoch [5/200], Hinge Loss: 0.0002588242059573531, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5\n",
      "Epoch [10/200], Hinge Loss: 0.0002171766827814281, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5375368923611111\n",
      "Epoch [15/200], Hinge Loss: 0.00021409873443190008, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5005208333333333\n",
      "Epoch [20/200], Hinge Loss: 0.00021322851534932852, Train_acc: , Accuracy: 0.5, ROC AUC: 0.6367420789930556\n",
      "Epoch [25/200], Hinge Loss: 0.0002155217225663364, Train_acc: , Accuracy: 0.5, ROC AUC: 0.6627007378472223\n",
      "Epoch [30/200], Hinge Loss: 0.0002097525430144742, Train_acc: , Accuracy: 0.5885416865348816, ROC AUC: 0.6654427083333334\n",
      "Epoch [35/200], Hinge Loss: 0.00020845630206167698, Train_acc: , Accuracy: 0.6223958134651184, ROC AUC: 0.6656998697916667\n",
      "Epoch [40/200], Hinge Loss: 0.00020636862609535456, Train_acc: , Accuracy: 0.6567708253860474, ROC AUC: 0.7451231553819444\n",
      "Epoch [45/200], Hinge Loss: 0.0002038786915363744, Train_acc: , Accuracy: 0.6343749761581421, ROC AUC: 0.7396408420138889\n",
      "Epoch [50/200], Hinge Loss: 0.0002027851005550474, Train_acc: , Accuracy: 0.6885416507720947, ROC AUC: 0.7602463107638889\n",
      "Epoch [55/200], Hinge Loss: 0.00020632856467273086, Train_acc: , Accuracy: 0.6869791746139526, ROC AUC: 0.751388888888889\n",
      "Epoch [60/200], Hinge Loss: 0.00020069314632564783, Train_acc: , Accuracy: 0.6937500238418579, ROC AUC: 0.7634559461805557\n",
      "Epoch [65/200], Hinge Loss: 0.00019826844800263643, Train_acc: , Accuracy: 0.6937500238418579, ROC AUC: 0.7608355034722223\n",
      "Epoch [70/200], Hinge Loss: 0.00019593923934735358, Train_acc: , Accuracy: 0.6395833492279053, ROC AUC: 0.7419585503472221\n",
      "Epoch [75/200], Hinge Loss: 0.00020675717678386718, Train_acc: , Accuracy: 0.6380208134651184, ROC AUC: 0.7339832899305555\n",
      "Epoch [80/200], Hinge Loss: 0.0002047840680461377, Train_acc: , Accuracy: 0.635937511920929, ROC AUC: 0.7388487413194444\n",
      "Epoch [85/200], Hinge Loss: 0.00020020836382173002, Train_acc: , Accuracy: 0.6354166865348816, ROC AUC: 0.7477555338541667\n",
      "Epoch [90/200], Hinge Loss: 0.00020068121375516057, Train_acc: , Accuracy: 0.6963541507720947, ROC AUC: 0.7639171006944443\n",
      "Epoch [95/200], Hinge Loss: 0.0001998846128117293, Train_acc: , Accuracy: 0.690625011920929, ROC AUC: 0.7637065972222221\n",
      "Epoch [100/200], Hinge Loss: 0.00019683653954416513, Train_acc: , Accuracy: 0.6963541507720947, ROC AUC: 0.7671028645833333\n",
      "Epoch [105/200], Hinge Loss: 0.0002009380841627717, Train_acc: , Accuracy: 0.6859375238418579, ROC AUC: 0.7529188368055555\n",
      "Epoch [110/200], Hinge Loss: 0.0001965140545507893, Train_acc: , Accuracy: 0.6947916746139526, ROC AUC: 0.7683962673611111\n",
      "Epoch [115/200], Hinge Loss: 0.00019600539235398173, Train_acc: , Accuracy: 0.6364583373069763, ROC AUC: 0.748615451388889\n",
      "Epoch [120/200], Hinge Loss: 0.00020197972480673343, Train_acc: , Accuracy: 0.675000011920929, ROC AUC: 0.760183376736111\n",
      "Epoch [125/200], Hinge Loss: 0.00019786079064942896, Train_acc: , Accuracy: 0.676562488079071, ROC AUC: 0.7499989149305555\n",
      "Epoch [130/200], Hinge Loss: 0.00019943530787713826, Train_acc: , Accuracy: 0.6875, ROC AUC: 0.7626193576388889\n",
      "Epoch [135/200], Hinge Loss: 0.00019665388390421867, Train_acc: , Accuracy: 0.6953125, ROC AUC: 0.7643912760416667\n",
      "Epoch [140/200], Hinge Loss: 0.00019487139070406556, Train_acc: , Accuracy: 0.6364583373069763, ROC AUC: 0.747170138888889\n",
      "Epoch [145/200], Hinge Loss: 0.0002034839999396354, Train_acc: , Accuracy: 0.6369791626930237, ROC AUC: 0.7565896267361111\n",
      "Epoch [150/200], Hinge Loss: 0.00019908997637685388, Train_acc: , Accuracy: 0.6802083253860474, ROC AUC: 0.7593695746527778\n",
      "Epoch [155/200], Hinge Loss: 0.00020189523638691753, Train_acc: , Accuracy: 0.6901041865348816, ROC AUC: 0.7672102864583334\n",
      "Epoch [160/200], Hinge Loss: 0.00019937913748435676, Train_acc: , Accuracy: 0.635937511920929, ROC AUC: 0.7359114583333333\n",
      "Epoch [165/200], Hinge Loss: 0.00019994843751192093, Train_acc: , Accuracy: 0.6380208134651184, ROC AUC: 0.7431825086805555\n",
      "Epoch [170/200], Hinge Loss: 0.00020411252626217902, Train_acc: , Accuracy: 0.6723958253860474, ROC AUC: 0.7530740017361112\n",
      "Epoch [175/200], Hinge Loss: 0.00020146710448898375, Train_acc: , Accuracy: 0.6932291388511658, ROC AUC: 0.7669672309027777\n",
      "Epoch [180/200], Hinge Loss: 0.00019950678688474, Train_acc: , Accuracy: 0.6380208134651184, ROC AUC: 0.7497694227430555\n",
      "Epoch [185/200], Hinge Loss: 0.00019932660507038236, Train_acc: , Accuracy: 0.6942708492279053, ROC AUC: 0.7654687500000001\n",
      "Epoch [190/200], Hinge Loss: 0.000196573615539819, Train_acc: , Accuracy: 0.6895833611488342, ROC AUC: 0.7624392361111112\n",
      "Epoch [195/200], Hinge Loss: 0.00019970284483861178, Train_acc: , Accuracy: 0.6807291507720947, ROC AUC: 0.7634244791666667\n",
      "Epoch [200/200], Hinge Loss: 0.00019746834004763514, Train_acc: , Accuracy: 0.6442708373069763, ROC AUC: 0.7572384982638889\n",
      "appending the predictions of IE\n",
      "Best accuracy for IE: 0.7683962673611111\n",
      "torch.Size([12778, 77]) <- col_data, torch.Size([12778, 768]) <- train_result\n",
      "Epoch [5/200], Hinge Loss: 0.000327204616041854, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5040608723958333\n",
      "Epoch [10/200], Hinge Loss: 0.0002465266443323344, Train_acc: , Accuracy: 0.4781250059604645, ROC AUC: 0.5034396701388889\n",
      "Epoch [15/200], Hinge Loss: 0.00025128686684183776, Train_acc: , Accuracy: 0.5, ROC AUC: 0.48554416232638886\n",
      "Epoch [20/200], Hinge Loss: 0.00023691845126450062, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5358610026041667\n",
      "Epoch [25/200], Hinge Loss: 0.0002378846111241728, Train_acc: , Accuracy: 0.512499988079071, ROC AUC: 0.5846055772569444\n",
      "Epoch [30/200], Hinge Loss: 0.00024166784714907408, Train_acc: , Accuracy: 0.5192708373069763, ROC AUC: 0.6759939236111111\n",
      "Epoch [35/200], Hinge Loss: 0.00022786614135839045, Train_acc: , Accuracy: 0.5510416626930237, ROC AUC: 0.7600135633680556\n",
      "Epoch [40/200], Hinge Loss: 0.00022781151346862316, Train_acc: , Accuracy: 0.5640624761581421, ROC AUC: 0.6969346788194444\n",
      "Epoch [45/200], Hinge Loss: 0.00022614671615883708, Train_acc: , Accuracy: 0.7114583253860474, ROC AUC: 0.7862076822916666\n",
      "Epoch [50/200], Hinge Loss: 0.00022715909290127456, Train_acc: , Accuracy: 0.7083333134651184, ROC AUC: 0.7828266059027778\n",
      "Epoch [55/200], Hinge Loss: 0.00022684971918351948, Train_acc: , Accuracy: 0.578125, ROC AUC: 0.7646256510416667\n",
      "Epoch [60/200], Hinge Loss: 0.00022372885723598301, Train_acc: , Accuracy: 0.6937500238418579, ROC AUC: 0.7744216579861112\n",
      "Epoch [65/200], Hinge Loss: 0.00023218296701088548, Train_acc: , Accuracy: 0.7083333134651184, ROC AUC: 0.7839670138888888\n",
      "Epoch [70/200], Hinge Loss: 0.00023334729485213757, Train_acc: , Accuracy: 0.7067708373069763, ROC AUC: 0.7850434027777776\n",
      "Epoch [75/200], Hinge Loss: 0.00023113536008168012, Train_acc: , Accuracy: 0.7098958492279053, ROC AUC: 0.7858919270833333\n",
      "Epoch [80/200], Hinge Loss: 0.0002289779658894986, Train_acc: , Accuracy: 0.6802083253860474, ROC AUC: 0.777763671875\n",
      "Epoch [85/200], Hinge Loss: 0.00022358266869559884, Train_acc: , Accuracy: 0.7088541388511658, ROC AUC: 0.7847699652777778\n",
      "Epoch [90/200], Hinge Loss: 0.0002246956864837557, Train_acc: , Accuracy: 0.706250011920929, ROC AUC: 0.7857595486111111\n",
      "Epoch [95/200], Hinge Loss: 0.00022448701201938093, Train_acc: , Accuracy: 0.7072916626930237, ROC AUC: 0.7857291666666666\n",
      "Epoch [100/200], Hinge Loss: 0.00022203507251106203, Train_acc: , Accuracy: 0.6536458134651184, ROC AUC: 0.7744818793402778\n",
      "Epoch [105/200], Hinge Loss: 0.00022611305757891387, Train_acc: , Accuracy: 0.7072916626930237, ROC AUC: 0.7847905815972223\n",
      "Epoch [110/200], Hinge Loss: 0.00022478948812931776, Train_acc: , Accuracy: 0.7036458253860474, ROC AUC: 0.7758409288194444\n",
      "Epoch [115/200], Hinge Loss: 0.00022174231708049774, Train_acc: , Accuracy: 0.706250011920929, ROC AUC: 0.7859016927083333\n",
      "Epoch [120/200], Hinge Loss: 0.00022296098177321255, Train_acc: , Accuracy: 0.6630208492279053, ROC AUC: 0.7751182725694444\n",
      "Epoch [125/200], Hinge Loss: 0.00022573486785404384, Train_acc: , Accuracy: 0.707812488079071, ROC AUC: 0.7884830729166667\n",
      "Epoch [130/200], Hinge Loss: 0.0002242191694676876, Train_acc: , Accuracy: 0.7104166746139526, ROC AUC: 0.7897580295138888\n",
      "Epoch [135/200], Hinge Loss: 0.00022568542044609785, Train_acc: , Accuracy: 0.707812488079071, ROC AUC: 0.7867393663194444\n",
      "Epoch [140/200], Hinge Loss: 0.00022448558593168855, Train_acc: , Accuracy: 0.7098958492279053, ROC AUC: 0.7842404513888889\n",
      "Epoch [145/200], Hinge Loss: 0.00022183275723364204, Train_acc: , Accuracy: 0.7104166746139526, ROC AUC: 0.7897743055555555\n",
      "Epoch [150/200], Hinge Loss: 0.00022378875291906297, Train_acc: , Accuracy: 0.7052083611488342, ROC AUC: 0.7870475260416665\n",
      "Epoch [155/200], Hinge Loss: 0.0002223025803687051, Train_acc: , Accuracy: 0.6911458373069763, ROC AUC: 0.7805414496527778\n",
      "Epoch [160/200], Hinge Loss: 0.0002219843736384064, Train_acc: , Accuracy: 0.6494791507720947, ROC AUC: 0.7774587673611112\n",
      "Epoch [165/200], Hinge Loss: 0.00022358822752721608, Train_acc: , Accuracy: 0.7036458253860474, ROC AUC: 0.7824338107638888\n",
      "Epoch [170/200], Hinge Loss: 0.00022388168144971132, Train_acc: , Accuracy: 0.703125, ROC AUC: 0.7835221354166667\n",
      "Epoch [175/200], Hinge Loss: 0.0002222376351710409, Train_acc: , Accuracy: 0.7098958492279053, ROC AUC: 0.7900217013888888\n",
      "Epoch [180/200], Hinge Loss: 0.000220956964767538, Train_acc: , Accuracy: 0.7088541388511658, ROC AUC: 0.7871082899305557\n",
      "Epoch [185/200], Hinge Loss: 0.0002209481317549944, Train_acc: , Accuracy: 0.7083333134651184, ROC AUC: 0.7856640625\n",
      "Epoch [190/200], Hinge Loss: 0.00022114267630968243, Train_acc: , Accuracy: 0.707812488079071, ROC AUC: 0.7868999565972222\n",
      "Epoch [195/200], Hinge Loss: 0.0002206680364906788, Train_acc: , Accuracy: 0.7088541388511658, ROC AUC: 0.7885785590277778\n",
      "Epoch [200/200], Hinge Loss: 0.00022412161342799664, Train_acc: , Accuracy: 0.7067708373069763, ROC AUC: 0.7871321614583334\n",
      "appending the predictions of SN\n",
      "Best accuracy for SN: 0.7907573784722223\n",
      "torch.Size([12778, 78]) <- col_data, torch.Size([12778, 768]) <- train_result\n",
      "Epoch [5/200], Hinge Loss: 0.00024262757506221533, Train_acc: , Accuracy: 0.5, ROC AUC: 0.4915418836805556\n",
      "Epoch [10/200], Hinge Loss: 0.00023123007849790156, Train_acc: , Accuracy: 0.4703125059604645, ROC AUC: 0.48833007812500007\n",
      "Epoch [15/200], Hinge Loss: 0.00022650112805422395, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5301041666666667\n",
      "Epoch [20/200], Hinge Loss: 0.00022301373246591538, Train_acc: , Accuracy: 0.48750001192092896, ROC AUC: 0.5597802734374999\n",
      "Epoch [25/200], Hinge Loss: 0.0002212424442404881, Train_acc: , Accuracy: 0.5114583373069763, ROC AUC: 0.5929578993055556\n",
      "Epoch [30/200], Hinge Loss: 0.0002188391808886081, Train_acc: , Accuracy: 0.5463541746139526, ROC AUC: 0.6588726128472222\n",
      "Epoch [35/200], Hinge Loss: 0.00021926422778051347, Train_acc: , Accuracy: 0.5963541865348816, ROC AUC: 0.6777528211805556\n",
      "Epoch [40/200], Hinge Loss: 0.00022085680393502116, Train_acc: , Accuracy: 0.6072916388511658, ROC AUC: 0.7659678819444444\n",
      "Epoch [45/200], Hinge Loss: 0.0002181640884373337, Train_acc: , Accuracy: 0.6343749761581421, ROC AUC: 0.7762999131944445\n",
      "Epoch [50/200], Hinge Loss: 0.00021672162984032184, Train_acc: , Accuracy: 0.6932291388511658, ROC AUC: 0.7853927951388888\n",
      "Epoch [55/200], Hinge Loss: 0.0002153533132513985, Train_acc: , Accuracy: 0.7213541865348816, ROC AUC: 0.8004893663194446\n",
      "Epoch [60/200], Hinge Loss: 0.00021420829580165446, Train_acc: , Accuracy: 0.734375, ROC AUC: 0.8154481336805556\n",
      "Epoch [65/200], Hinge Loss: 0.00021306569396983832, Train_acc: , Accuracy: 0.737500011920929, ROC AUC: 0.8199164496527779\n",
      "Epoch [70/200], Hinge Loss: 0.00021332150208763778, Train_acc: , Accuracy: 0.7364583611488342, ROC AUC: 0.8198730468750001\n",
      "Epoch [75/200], Hinge Loss: 0.00021279264183249325, Train_acc: , Accuracy: 0.7364583611488342, ROC AUC: 0.8202907986111112\n",
      "Epoch [80/200], Hinge Loss: 0.00021160044707357883, Train_acc: , Accuracy: 0.7385416626930237, ROC AUC: 0.8209814453125\n",
      "Epoch [85/200], Hinge Loss: 0.0002117355033988133, Train_acc: , Accuracy: 0.7380208373069763, ROC AUC: 0.8206857638888889\n",
      "Epoch [90/200], Hinge Loss: 0.00021246423420961946, Train_acc: , Accuracy: 0.7359374761581421, ROC AUC: 0.8191688368055556\n",
      "Epoch [95/200], Hinge Loss: 0.00021217321045696735, Train_acc: , Accuracy: 0.7359374761581421, ROC AUC: 0.820090060763889\n",
      "Epoch [100/200], Hinge Loss: 0.00021106493659317493, Train_acc: , Accuracy: 0.739062488079071, ROC AUC: 0.8200000000000001\n",
      "Epoch [105/200], Hinge Loss: 0.00021101365564391017, Train_acc: , Accuracy: 0.7369791865348816, ROC AUC: 0.8212120225694444\n",
      "Epoch [110/200], Hinge Loss: 0.0002099656849168241, Train_acc: , Accuracy: 0.7369791865348816, ROC AUC: 0.8184309895833333\n",
      "Epoch [115/200], Hinge Loss: 0.00020948922610841691, Train_acc: , Accuracy: 0.7364583611488342, ROC AUC: 0.8192252604166667\n",
      "Epoch [120/200], Hinge Loss: 0.00021097835269756615, Train_acc: , Accuracy: 0.7385416626930237, ROC AUC: 0.8209993489583334\n",
      "Epoch [125/200], Hinge Loss: 0.0002112908405251801, Train_acc: , Accuracy: 0.7369791865348816, ROC AUC: 0.8211371527777779\n",
      "Epoch [130/200], Hinge Loss: 0.00020918407244607806, Train_acc: , Accuracy: 0.7369791865348816, ROC AUC: 0.8193315972222222\n",
      "Epoch [135/200], Hinge Loss: 0.00021170206309761852, Train_acc: , Accuracy: 0.7380208373069763, ROC AUC: 0.8212055121527777\n",
      "Epoch [140/200], Hinge Loss: 0.00020964053692296147, Train_acc: , Accuracy: 0.739062488079071, ROC AUC: 0.8177115885416666\n",
      "Epoch [145/200], Hinge Loss: 0.00020873657194897532, Train_acc: , Accuracy: 0.7380208373069763, ROC AUC: 0.8202918836805555\n",
      "Epoch [150/200], Hinge Loss: 0.00021237306646071374, Train_acc: , Accuracy: 0.7359374761581421, ROC AUC: 0.82083984375\n",
      "Epoch [155/200], Hinge Loss: 0.0002089772024191916, Train_acc: , Accuracy: 0.7380208373069763, ROC AUC: 0.8207020399305555\n",
      "Epoch [160/200], Hinge Loss: 0.00021088185894768685, Train_acc: , Accuracy: 0.7359374761581421, ROC AUC: 0.8213313802083334\n",
      "Epoch [165/200], Hinge Loss: 0.0002089118934236467, Train_acc: , Accuracy: 0.731249988079071, ROC AUC: 0.8147960069444444\n",
      "Epoch [170/200], Hinge Loss: 0.00021091515372972935, Train_acc: , Accuracy: 0.7369791865348816, ROC AUC: 0.8179188368055557\n",
      "Epoch [175/200], Hinge Loss: 0.0002086354943457991, Train_acc: , Accuracy: 0.737500011920929, ROC AUC: 0.8180891927083334\n",
      "Epoch [180/200], Hinge Loss: 0.00020998898253310472, Train_acc: , Accuracy: 0.7307291626930237, ROC AUC: 0.8155685763888889\n",
      "Epoch [185/200], Hinge Loss: 0.0002096177195198834, Train_acc: , Accuracy: 0.7354166507720947, ROC AUC: 0.8204014756944445\n",
      "Epoch [190/200], Hinge Loss: 0.00020851707085967064, Train_acc: , Accuracy: 0.7385416626930237, ROC AUC: 0.8185883246527779\n",
      "Epoch [195/200], Hinge Loss: 0.00021202545030973852, Train_acc: , Accuracy: 0.7380208373069763, ROC AUC: 0.8213769531250001\n",
      "Epoch [200/200], Hinge Loss: 0.0002120206772815436, Train_acc: , Accuracy: 0.7395833134651184, ROC AUC: 0.822121310763889\n",
      "appending the predictions of TF\n",
      "Best accuracy for TF: 0.822121310763889\n",
      "torch.Size([12778, 79]) <- col_data, torch.Size([12778, 768]) <- train_result\n",
      "Epoch [5/200], Hinge Loss: 0.0002418115473119542, Train_acc: , Accuracy: 0.49531251192092896, ROC AUC: 0.5056499565972222\n",
      "Epoch [10/200], Hinge Loss: 0.0002319507475476712, Train_acc: , Accuracy: 0.5177083611488342, ROC AUC: 0.513752170138889\n",
      "Epoch [15/200], Hinge Loss: 0.0002335949830012396, Train_acc: , Accuracy: 0.49687498807907104, ROC AUC: 0.47226508246527776\n",
      "Epoch [20/200], Hinge Loss: 0.0002298687759321183, Train_acc: , Accuracy: 0.5, ROC AUC: 0.5237586805555556\n",
      "Epoch [25/200], Hinge Loss: 0.00022786189219914377, Train_acc: , Accuracy: 0.4765625, ROC AUC: 0.534748263888889\n",
      "Epoch [30/200], Hinge Loss: 0.00022811302915215492, Train_acc: , Accuracy: 0.5895833373069763, ROC AUC: 0.6352880859375\n",
      "Epoch [35/200], Hinge Loss: 0.0002277155581396073, Train_acc: , Accuracy: 0.6208333373069763, ROC AUC: 0.6834315321180555\n",
      "Epoch [40/200], Hinge Loss: 0.0002263508504256606, Train_acc: , Accuracy: 0.621874988079071, ROC AUC: 0.7329372829861112\n",
      "Epoch [45/200], Hinge Loss: 0.00022689031902700663, Train_acc: , Accuracy: 0.6223958134651184, ROC AUC: 0.7387934027777777\n",
      "Epoch [50/200], Hinge Loss: 0.00022615480702370405, Train_acc: , Accuracy: 0.7114583253860474, ROC AUC: 0.7598383246527778\n",
      "Epoch [55/200], Hinge Loss: 0.0002262595808133483, Train_acc: , Accuracy: 0.7802083492279053, ROC AUC: 0.8698784722222223\n",
      "Epoch [60/200], Hinge Loss: 0.00022661661205347627, Train_acc: , Accuracy: 0.7427083253860474, ROC AUC: 0.8042068142361112\n",
      "Epoch [65/200], Hinge Loss: 0.00022624622215516865, Train_acc: , Accuracy: 0.8104166388511658, ROC AUC: 0.8929275173611112\n",
      "Epoch [70/200], Hinge Loss: 0.00022547997650690377, Train_acc: , Accuracy: 0.8062499761581421, ROC AUC: 0.8920247395833333\n",
      "Epoch [75/200], Hinge Loss: 0.00022577869822271168, Train_acc: , Accuracy: 0.7984374761581421, ROC AUC: 0.8875911458333334\n",
      "Epoch [80/200], Hinge Loss: 0.0002261292829643935, Train_acc: , Accuracy: 0.7755208611488342, ROC AUC: 0.8654654947916667\n",
      "Epoch [85/200], Hinge Loss: 0.0002252640697406605, Train_acc: , Accuracy: 0.7739583253860474, ROC AUC: 0.8623480902777778\n",
      "Epoch [90/200], Hinge Loss: 0.0002274188445881009, Train_acc: , Accuracy: 0.7786458134651184, ROC AUC: 0.8535536024305557\n",
      "Epoch [95/200], Hinge Loss: 0.00022585003171116114, Train_acc: , Accuracy: 0.8062499761581421, ROC AUC: 0.8912858072916667\n",
      "Epoch [100/200], Hinge Loss: 0.00022492586867883801, Train_acc: , Accuracy: 0.8036458492279053, ROC AUC: 0.893603515625\n",
      "Epoch [105/200], Hinge Loss: 0.00022507048561237752, Train_acc: , Accuracy: 0.8057291507720947, ROC AUC: 0.8875260416666666\n",
      "Epoch [110/200], Hinge Loss: 0.0002247232769150287, Train_acc: , Accuracy: 0.8119791746139526, ROC AUC: 0.8934038628472223\n",
      "Epoch [115/200], Hinge Loss: 0.00022473902208730578, Train_acc: , Accuracy: 0.807812511920929, ROC AUC: 0.8936273871527778\n",
      "Epoch [120/200], Hinge Loss: 0.00022479117615148425, Train_acc: , Accuracy: 0.8031250238418579, ROC AUC: 0.8806401909722221\n",
      "Epoch [125/200], Hinge Loss: 0.0002251667610835284, Train_acc: , Accuracy: 0.8020833134651184, ROC AUC: 0.8894401041666666\n",
      "Epoch [130/200], Hinge Loss: 0.00022480271582026035, Train_acc: , Accuracy: 0.807812511920929, ROC AUC: 0.8851974826388889\n",
      "Epoch [135/200], Hinge Loss: 0.00022528821136802435, Train_acc: , Accuracy: 0.7916666865348816, ROC AUC: 0.8763378906250001\n",
      "Epoch [140/200], Hinge Loss: 0.00022354722023010254, Train_acc: , Accuracy: 0.8104166388511658, ROC AUC: 0.8927582465277778\n",
      "Epoch [145/200], Hinge Loss: 0.0002249673125334084, Train_acc: , Accuracy: 0.765625, ROC AUC: 0.8370073784722223\n",
      "Epoch [150/200], Hinge Loss: 0.00022337150585372, Train_acc: , Accuracy: 0.8130208253860474, ROC AUC: 0.8898687065972223\n",
      "Epoch [155/200], Hinge Loss: 0.0002227664226666093, Train_acc: , Accuracy: 0.7807291746139526, ROC AUC: 0.8600054253472221\n",
      "Epoch [160/200], Hinge Loss: 0.00022362580057233572, Train_acc: , Accuracy: 0.807812511920929, ROC AUC: 0.8844108072916665\n",
      "Epoch [165/200], Hinge Loss: 0.0002221216564066708, Train_acc: , Accuracy: 0.8104166388511658, ROC AUC: 0.8946408420138889\n",
      "Epoch [170/200], Hinge Loss: 0.00022292984067462385, Train_acc: , Accuracy: 0.770312488079071, ROC AUC: 0.843786892361111\n",
      "Epoch [175/200], Hinge Loss: 0.00022228920715861022, Train_acc: , Accuracy: 0.8026041388511658, ROC AUC: 0.8789214409722221\n",
      "Epoch [180/200], Hinge Loss: 0.00022217637160792947, Train_acc: , Accuracy: 0.8114583492279053, ROC AUC: 0.8894379340277778\n",
      "Epoch [185/200], Hinge Loss: 0.00022259763500187546, Train_acc: , Accuracy: 0.8046875, ROC AUC: 0.8935980902777777\n",
      "Epoch [190/200], Hinge Loss: 0.00022053623979445547, Train_acc: , Accuracy: 0.7802083492279053, ROC AUC: 0.8617686631944443\n",
      "Epoch [195/200], Hinge Loss: 0.00021974033734295517, Train_acc: , Accuracy: 0.8010416626930237, ROC AUC: 0.8836480034722222\n",
      "Epoch [200/200], Hinge Loss: 0.00021900465071666986, Train_acc: , Accuracy: 0.8119791746139526, ROC AUC: 0.8833539496527777\n",
      "appending the predictions of JP\n",
      "Best accuracy for JP: 0.8962489149305556\n"
     ]
    }
   ],
   "source": [
    "# Train all\n",
    "# suppress the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "test_number = 201\n",
    "epochs = 200\n",
    "import csv\n",
    "MBTI = ['IE', 'SN', 'TF', 'JP']\n",
    "set_random(42)\n",
    "best_accuracies = {}\n",
    "result_dir = f'./result_{test_number}_{model_name.replace(\"/\", \"_\")}'\n",
    "preds = []\n",
    "test_preds = []\n",
    "train_label = train_df['MBTI'].map(convert_mbti_to_label)\n",
    "train_label_2d = np.array(train_label.tolist())\n",
    "test_label = test_df['MBTI'].map(convert_mbti_to_label)\n",
    "test_label_2d = np.array(test_label.tolist())\n",
    "for i in range(4):\n",
    "    label_idx = i\n",
    "    # convert a pandas series whose data values are list -> to a 2D numpy array\n",
    "    train_label_1d = train_label_2d[:, label_idx]\n",
    "    test_label_1d = test_label_2d[:, label_idx]\n",
    "    # preds_tensor = torch.tensor(preds).cuda(0)\n",
    "    # col_data_merged = torch.cat([col_data, test_col_data.cuda(0)], dim=0)\n",
    "    # labels_merged = np.concatenate([train_label_1d, test_label_1d], axis=0)\n",
    "    # # test_result = test_result.cuda(0)\n",
    "    # result_merged = torch.cat([train_result, test_result.cuda(0)], dim=0)\n",
    "    max_auc, accuracy, train_prob_preds, test_prob_preds = main(label_idx, epochs, MBTI[i], test_number, col_data, test_col_data, train_label_1d, test_label_1d, train_result, test_result)\n",
    "    train_label_1d = torch.tensor(train_label_1d).cuda(0).reshape(-1, 1)\n",
    "    test_label_1d = torch.tensor(test_label_1d).cuda(0).reshape(-1, 1)\n",
    "    col_data = torch.cat([col_data, train_label_1d], dim=1)\n",
    "    col_data = col_data.float()\n",
    "    test_col_data = torch.cat([test_col_data.cuda(0), test_label_1d], dim=1)\n",
    "    test_col_data = test_col_data.float()\n",
    "    # preds.append(train_prob_preds)\n",
    "    # test_preds.append(test_prob_preds)\n",
    "    print('appending the predictions of', MBTI[i])\n",
    "    best_accuracies[MBTI[i]] = max_auc\n",
    "    print(f'Best accuracy for {MBTI[i]}: {max_auc}')\n",
    "\n",
    "# print(best_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0377d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IE': 0.7683962673611111,\n",
       " 'SN': 0.7907573784722223,\n",
       " 'TF': 0.822121310763889,\n",
       " 'JP': 0.8962489149305556}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee53b620",
   "metadata": {},
   "source": [
    "Even though the performance of the model seem very high here, when we submitted it to kaggle it got very low accuracy.\n",
    "This is because the model is overfitting the training data because it receives previous labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
